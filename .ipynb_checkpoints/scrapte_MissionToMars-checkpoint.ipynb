{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies: import necessary libraries\n",
    "from selenium import webdriver  # Web Scraping Framework # selenium webdriver API\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome('windows/chromedriver')  # Browerser Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <1>NASA Mars News:\n",
    "# Scrape the NASA Mars News Site and collect the {latest} News [Title] and [Paragraph Text]. \n",
    "# Assign the text to variables that you can reference later.\n",
    "\n",
    "# URL of page to be scraped\n",
    "url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'\n",
    "browser.get(url)\n",
    "\n",
    "# Scrape page into Soup\n",
    "html = browser.page_source\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = bs(html, 'html.parser')\n",
    "# Examine the results, then determine element that contains sought info\n",
    "# print(soup.body.prettify())\n",
    "\n",
    "# Extract the [title] of the HTML document\n",
    "# Access the latest news with CSS selectors\n",
    "news_list = soup.find_all('div', class_='list_text') # find() is the later one, find_all() is all\n",
    "# print(news_list[0].prettify())\n",
    "latestnews_title = news_list[0].find('div', class_='content_title').a.text #\"NASA's Next Mars Mission to Investigate Interior of Red Planet\"\n",
    "latestnews_p = news_list[0].find('div', class_='article_teaser_body').text#\"Preparation of NASA's next spac...\"\n",
    "# print(latestnews_title)\n",
    "# print(latestnews_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.jpl.nasa.gov/spaceimages/images/mediumsize/PIA18292_ip.jpg\n"
     ]
    }
   ],
   "source": [
    "# <2> JPL Mars Space Images - Featured Image : \n",
    "# Use Selenium to navigate the site and find the image url for the {current Featured Mars Image} and assign the [url string] to a variable called featured_image_url.\n",
    "# Make sure to find the image url to the {full size .jpg image}.\n",
    "# Make sure to save a complete url string for this image.\n",
    "# featured_image_url = 'https://www.jpl.nasa.gov/spaceimages/images/largesize/PIA16225_hires.jpg'\n",
    "\n",
    "# URL of page to be scraped\n",
    "url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "browser.get(url)\n",
    "\n",
    "# Scrape page into Soup\n",
    "html = browser.page_source\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = bs(html, 'html.parser')\n",
    "# Examine the results, then determine element that contains sought info\n",
    "# print(soup.body.prettify())\n",
    "imageurl_list = soup.find_all('footer')\n",
    "featured_image_url = 'https://www.jpl.nasa.gov' + imageurl_list[0].find('a', class_='button fancybox')['data-fancybox-href']\n",
    "print(featured_image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all paragraph elements# Retrieve all elements that contain book information\n",
    "# results are returned as an iterable list\n",
    "\n",
    "\n",
    "\n",
    "# A blank list to hold the headlines\n",
    "category_list = []\n",
    "url_list = []\n",
    "book_url_list = []\n",
    "# Loop through results to retrieve article title, header, and timestamp of article\n",
    "for category in categories:\n",
    "    # Access the thread's text content\n",
    "    # Clean up the text\n",
    "    title = category.text.strip()\n",
    "    category_list.append(title)\n",
    "    # Identify and return link to listing \n",
    "    # the href attribute with bracket notation# Use Beautiful Soup's find() method to navigate and retrieve attributes\n",
    "    book_url = category.find('a')['href']\n",
    "    url_list.append(book_url)\n",
    "\n",
    "book_url_list = ['http://books.toscrape.com/' + url for url in url_list]\n",
    "\n",
    "titles_and_urls = zip(category_list, book_url_list)\n",
    "# Error handling\n",
    "try:\n",
    "    for title_url in titles_and_urls:\n",
    "         # Click the 'Next' button on each page\n",
    "        browser.find_element_by_partial_link_text('next').click()   \n",
    "except NoSuchElementException:\n",
    "    print(\"Scraping Complete\")\n",
    "    # Close the browser after scraping\n",
    "    browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InSight sol 319 (2019-10-19) low -101.5ºC (-150.7ºF) high -25.5ºC (-13.9ºF)\n",
      "winds from the SSE at 4.6 m/s (10.4 mph) gusting to 18.4 m/s (41.2 mph)\n",
      "pressure at 7.10 hPapic.twitter.com/gdBUdujdVM\n"
     ]
    }
   ],
   "source": [
    "# <3> Mars Weather\n",
    "# scrape the {latest Mars weather tweet} from the page. Save the tweet text for the weather report as a variable called mars_weather.\n",
    "# mars_weather = 'Sol 1801 (Aug 30, 2017), Sunny, high -21C/-5F, low -80C/-112F, pressure at 8.82 hPa, daylight 06:09-17:55'\n",
    "\n",
    "# URL of page to be scraped\n",
    "url = 'https://twitter.com/marswxreport?lang=en'\n",
    "browser.get(url)\n",
    "# Scrape page into Soup\n",
    "html = browser.page_source\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = bs(html, 'html.parser')\n",
    "# Examine the results, then determine element that contains sought info\n",
    "# print(soup.body.prettify())\n",
    "tweet_list = soup.find_all('div',class_='js-tweet-text-container')\n",
    "\n",
    "latestmarsweather_tweet = tweet_list[0].find('p').text\n",
    "print(latestmarsweather_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <4> Mars Facts\n",
    "# use Pandas to scrape the {table} containing facts about the planet including Diameter, Mass, etc.\n",
    "# Use Pandas to convert the data to a HTML table string.\n",
    "\n",
    "# URL of page to be scraped\n",
    "url = 'https://space-facts.com/mars/'\n",
    "tables = pd.read_html(url)\n",
    "df = tables[1]  # the second table\n",
    "df.columns = ['fact', 'value']\n",
    "# df = df.iloc[2:] # clean\n",
    "df.set_index('fact', inplace=True)\n",
    "html_table = df.to_html()\n",
    "html_table.replace('\\n', '')\n",
    "# df.to_html('table.html')\n",
    "# print((html_table))\n",
    "soup = bs(html_table, 'html.parser').tbody\n",
    "# print(soup.prettify())\n",
    "tablerow_list = soup.find_all('tr')\n",
    "tablerowth_list = soup.find_all('th')\n",
    "# print(tablerowth_list)\n",
    "tablerowtd_list = soup.find_all('td')\n",
    "# print(tablerowtd_list)\n",
    "    # # == has index == # #\n",
    "    # # <table border=\"1\" class=\"dataframe\">\n",
    "    # #   <thead>   # two columns shold be one row, two rows??? because manual setted index\n",
    "    # #     <tr style=\"text-align: right;\">\n",
    "    # #       <th></th>\n",
    "    # #       <th>value</th>\n",
    "    # #     </tr>\n",
    "    # #     <tr>\n",
    "    # #       <th>fact</th>\n",
    "    # #       <th></th>\n",
    "    # #     </tr>\n",
    "    # #   </thead>\n",
    "    # #   <tbody>  # total 9 rows\n",
    "    # #     <tr>\n",
    "    # #       <th>Equatorial Diameter:</th>\n",
    "    # #       <td>6,792 km</td>\n",
    "    # #     </tr>\n",
    "    # #     ....\n",
    "    # #     <tr>\n",
    "    # #       <th>Recorded By:</th>\n",
    "    # #       <td>Egyptian astronomers</td>\n",
    "    # #     </tr>\n",
    "    # #   </tbody>\n",
    "    # # </table>\n",
    "\n",
    "    # # == no index == # #\n",
    "    # # <table border=\"1\" class=\"dataframe\">\n",
    "    # #   <thead>   #  one row three columns ?? because default index\n",
    "    # #     <tr style=\"text-align: right;\">\n",
    "    # #       <th></th>\n",
    "    # #       <th>fact</th>\n",
    "    # #       <th>value</th>\n",
    "    # #     </tr>\n",
    "    # #   </thead>\n",
    "    # #   <tbody>\n",
    "    # #     <tr>\n",
    "    # #       <th>0</th>\n",
    "    # #       <td>Equatorial Diameter:</td>\n",
    "    # #       <td>6,792 km</td>\n",
    "    # #     </tr>\n",
    "\n",
    "\n",
    "tablerowthvl_list=[]\n",
    "tablerowtdvl_list=[]\n",
    "tablerowthvlvl_list=[]\n",
    "tablerowtdvlvl_list=[]\n",
    "\n",
    "for r in tablerowth_list:\n",
    "    n = str(r)[4:]\n",
    "    tablerowthvl_list.append(n)\n",
    "for r in tablerowthvl_list:\n",
    "    n = str(r)[:-6]\n",
    "    tablerowthvlvl_list.append(n)    \n",
    "# print(tablerowthvlvl_list)\n",
    "\n",
    "for r in tablerowtd_list:\n",
    "    n = str(r)[4:]\n",
    "    tablerowtdvl_list.append(n)\n",
    "for r in tablerowtdvl_list:\n",
    "    n = str(r)[:-6]\n",
    "    tablerowtdvlvl_list.append(n)    \n",
    "# print(tablerowtdvlvl_list)\n",
    "\n",
    "th_and_td = zip(tablerowthvlvl_list,tablerowtdvlvl_list)\n",
    "# print(tuple(th_and_td))\n",
    "\n",
    "thtd_dict = {}\n",
    "thtd_dictlist =[]\n",
    "for i in th_and_td:\n",
    "    l=list(i)\n",
    "#     print(l)\n",
    "    thtd_dict[\"fact\"] = l[0]\n",
    "    thtd_dict[\"value\"] =l[1]\n",
    "#     print(thtd_dict)\n",
    "#     print(thtd_dictlist)\n",
    "    thtd_dictlist.append(thtd_dict.copy())  # !!.copy() ??\n",
    "#     print(thtd_dictlist)\n",
    "# print(thtd_dictlist)\n",
    "  \n",
    "# print(thtd_dictlist[0][\"fact\"]) \n",
    "# print(thtd_dictlist[0][\"value\"])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/search/map/Mars/Viking/cerberus_enhanced\n",
      "[{'title': 'Cerberus Hemisphere Enhanced', 'img_url': '/search/map/Mars/Viking/cerberus_enhanced'}, {'title': 'Schiaparelli Hemisphere Enhanced', 'img_url': '/search/map/Mars/Viking/schiaparelli_enhanced'}, {'title': 'Syrtis Major Hemisphere Enhanced', 'img_url': '/search/map/Mars/Viking/syrtis_major_enhanced'}, {'title': 'Valles Marineris Hemisphere Enhanced', 'img_url': '/search/map/Mars/Viking/valles_marineris_enhanced'}]\n"
     ]
    }
   ],
   "source": [
    "# <5> Mars Hemispheres\n",
    "# Jason: Looks like the for the week 12 HW that the Mars Hemispheres link (https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars) is unreliable. If it is not working for you, then choose 4 images from this link instead: https://www.usgs.gov/centers/astrogeology-science-center/multimedia (edited) \n",
    "# obtain {high} resolution images for each of Mar's hemispheres.\n",
    "# You will need to {{click each of the links}} to the hemispheres in order to find the image url to the {full resolution image}.\n",
    "# Save both the image url string for the full resolution hemisphere image, and the Hemisphere title containing the hemisphere name. Use a Python dictionary to store the data using the keys img_url and title.\n",
    "# Append the dictionary with the image url string and the hemisphere title to a list. This list will contain one dictionary for each hemisphere.\n",
    "# hemisphere_image_urls = [\n",
    "#     {\"title\": \"Valles Marineris Hemisphere\", \"img_url\": \"...\"},\n",
    "#     {\"title\": \"Cerberus Hemisphere\", \"img_url\": \"...\"},\n",
    "#     {\"title\": \"Schiaparelli Hemisphere\", \"img_url\": \"...\"},\n",
    "#     {\"title\": \"Syrtis Major Hemisphere\", \"img_url\": \"...\"},\n",
    "# ]\n",
    "\n",
    "# URL of page to be scraped\n",
    "url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "browser.get(url)\n",
    "# Scrape page into Soup\n",
    "html = browser.page_source\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = bs(html, 'html.parser')\n",
    "# Examine the results, then determine element that contains sought info\n",
    "# print(soup.body.prettify())\n",
    "hemisphere_dictlist = []\n",
    "hemisphere_dict = {}\n",
    "item_list = soup.find_all('div', class_=\"item\")\n",
    "# click each of the links to the hemispheres in order to find the image url to the {full resolution image ???\n",
    "for m in item_list:\n",
    "    hemisphere_dict[\"title\"] = m.div.a.h3.text\n",
    "    hemisphere_dict[\"img_url\"] = m.div.a[\"href\"]\n",
    "    hemisphere_dictlist.append(hemisphere_dict.copy())\n",
    "print(hemisphereimageurls_dictlist[0][\"img_url\"])\n",
    "print(hemisphereimageurls_dictlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape page into Soup\n",
    "html = browser.page_source\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = bs(html, 'html.parser')\n",
    "# Examine the results, then determine element that contains sought info\n",
    "print(soup.prettify())\n",
    "\n",
    "# Extract the title of the HTML document\n",
    "# Access the thread with CSS selectors\n",
    "sidebar = soup.find('ul', class_='nav-list')\n",
    "# Extract all paragraph elements# Retrieve all elements that contain book information\n",
    "# results are returned as an iterable list\n",
    "categories = sidebar.find_all('li')\n",
    "# A blank list to hold the headlines\n",
    "category_list = []\n",
    "url_list = []\n",
    "book_url_list = []\n",
    "# Loop through results to retrieve article title, header, and timestamp of article\n",
    "for category in categories:\n",
    "    # Access the thread's text content\n",
    "    # Clean up the text\n",
    "    title = category.text.strip()\n",
    "    category_list.append(title)\n",
    "    # Identify and return link to listing \n",
    "    # the href attribute with bracket notation# Use Beautiful Soup's find() method to navigate and retrieve attributes\n",
    "    book_url = category.find('a')['href']\n",
    "    url_list.append(book_url)\n",
    "\n",
    "book_url_list = ['http://books.toscrape.com/' + url for url in url_list]\n",
    "\n",
    "titles_and_urls = zip(category_list, book_url_list)\n",
    "# Error handling\n",
    "try:\n",
    "    for title_url in titles_and_urls:\n",
    "         # Click the 'Next' button on each page\n",
    "        browser.find_element_by_partial_link_text('next').click()   \n",
    "except NoSuchElementException:\n",
    "    print(\"Scraping Complete\")\n",
    "    # Close the browser after scraping\n",
    "    browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === selenium vs requests ===\n",
    "# import requests\n",
    "# # Retrieve page with the requests module\n",
    "# response = requests.get(url)\n",
    "\n",
    "# <1>https://www.accordbox.com/blog/web-scraping-framework-review-scrapy-vs-selenium/\n",
    "# Selenium is a framework which is designed to automate test for web applications. It provides a way for developer to write tests in a number of popular programming languages such as C#, Java, Python, Ruby, etc. The tests writen by developer can again most web browsers such as Chrome, IE and Firefox.\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "# driver = webdriver.Firefox()\n",
    "# driver.get(\"http://www.python.org\")\n",
    "# assert \"Python\" in driver.title\n",
    "# elem = driver.find_element_by_name(\"q\")\n",
    "# elem.send_keys(\"selenium\")\n",
    "# elem.send_keys(Keys.RETURN)\n",
    "# assert \"Google\" in driver.title\n",
    "# driver.close()\n",
    "# <2>https://tryolabs.com/blog/2017/11/22/requestium-integration-layer-requests-selenium-web-automation/\n",
    "# use Requests, the beloved Python HTTP library, for simple sites; and Selenium, the popular browser automation tool, for sites that make heavy use of Javascript. \n",
    "# <3>https://stackoverflow.com/questions/46746085/requests-vs-selenium-python   requests : can't handle the JS   \n",
    "# <4>https://www.blackhatworld.com/seo/http-requests-vs-selenium.934475/\n",
    "# for heavy javascript sites use solenium else use http web request.\n",
    "# requests is faster than selenium\n",
    "# selenium  browser based automation \n",
    "# selenium  render the entire webpage\n",
    "# javascript heavy sites can be a easier when using an automated browser but there sooo slooow\n",
    "# http request cannot \"click\" like selenium can. So you cant really automate everything.\n",
    "# Selenium is design for test with browser automation, in resume for html parse, and python requests is design for http request in general.\n",
    "# So, for use without any browser automation or html parse, use requests, otherwise use selenium.\n",
    "# always go with requests, selenium only last resort if you don't know what you need to do with requests to get where you want. I can count with my hand the websites with javascript that there was no way to scrape without a browser or something running javascript, usually javascript stuff that runs while you're on a login page and goes threw some obfuscated javascript and data gets encrypted and you need that encrypted value to make a requests(i've seen this in amazon and linkedin login, you can login to linkedin without javascript activated). But hey, you can just open selenium with that particular page and go on with requests with the cookie value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
