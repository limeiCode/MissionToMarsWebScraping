{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies: import necessary libraries\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === selenium vs requests ===\n",
    "# import requests\n",
    "# # Retrieve page with the requests module\n",
    "# response = requests.get(url)\n",
    "# https://tryolabs.com/blog/2017/11/22/requestium-integration-layer-requests-selenium-web-automation/\n",
    "# use Requests, the beloved Python HTTP library, for simple sites; and Selenium, the popular browser automation tool, for sites that make heavy use of Javascript. \n",
    "# requests : can't handle the JS   https://stackoverflow.com/questions/46746085/requests-vs-selenium-python\n",
    "# https://www.blackhatworld.com/seo/http-requests-vs-selenium.934475/\n",
    "# for heavy javascript sites use solenium else use http web request.\n",
    "# requests is faster than selenium\n",
    "# selenium  browser based automation \n",
    "# selenium  render the entire webpage\n",
    "# javascript heavy sites can be a easier when using an automated browser but there sooo slooow\n",
    "# http request cannot \"click\" like selenium can. So you cant really automate everything.\n",
    "# Selenium is design for test with browser automation, in resume for html parse, and python requests is design for http request in general.\n",
    "# So, for use without any browser automation or html parse, use requests, otherwise use selenium.\n",
    "# always go with requests, selenium only last resort if you don't know what you need to do with requests to get where you want. I can count with my hand the websites with javascript that there was no way to scrape without a browser or something running javascript, usually javascript stuff that runs while you're on a login page and goes threw some obfuscated javascript and data gets encrypted and you need that encrypted value to make a requests(i've seen this in amazon and linkedin login, you can login to linkedin without javascript activated). But hey, you can just open selenium with that particular page and go on with requests with the cookie value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome('windows/chromedriver')\n",
    "# URL of page to be scraped\n",
    "url = 'http://books.toscrape.com/'\n",
    "browser.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Complete\n"
     ]
    }
   ],
   "source": [
    "# Scrape page into Soup\n",
    "html = browser.page_source\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "# Examine the results, then determine element that contains sought info\n",
    "print(soup.prettify())\n",
    "\n",
    "# Extract the title of the HTML document\n",
    "# Access the thread with CSS selectors\n",
    "sidebar = soup.find('ul', class_='nav-list')\n",
    "# Extract all paragraph elements# Retrieve all elements that contain book information\n",
    "# results are returned as an iterable list\n",
    "categories = sidebar.find_all('li')\n",
    "# A blank list to hold the headlines\n",
    "category_list = []\n",
    "url_list = []\n",
    "book_url_list = []\n",
    "# Loop through results to retrieve article title, header, and timestamp of article\n",
    "for category in categories:\n",
    "    # Access the thread's text content\n",
    "    # Clean up the text\n",
    "    title = category.text.strip()\n",
    "    category_list.append(title)\n",
    "    # Identify and return link to listing \n",
    "    # the href attribute with bracket notation# Use Beautiful Soup's find() method to navigate and retrieve attributes\n",
    "    book_url = category.find('a')['href']\n",
    "    url_list.append(book_url)\n",
    "\n",
    "book_url_list = ['http://books.toscrape.com/' + url for url in url_list]\n",
    "\n",
    "titles_and_urls = zip(category_list, book_url_list)\n",
    "# Error handling\n",
    "try:\n",
    "    for title_url in titles_and_urls:\n",
    "         # Click the 'Next' button on each page\n",
    "        browser.find_element_by_partial_link_text('next').click()   \n",
    "except NoSuchElementException:\n",
    "    print(\"Scraping Complete\")\n",
    "    # Close the browser after scraping\n",
    "    browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
